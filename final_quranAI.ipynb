{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af5e0f0-8ff0-4c72-a28f-971e6a27c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339e0601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 474 total docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcd8cabce8343cc9bf02aff5cc2f65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing completed.\n"
     ]
    }
   ],
   "source": [
    "# === Paths ===\n",
    "emotion_dir = \"emotions\"\n",
    "ibadah_dir = \"ibadah\"\n",
    "\n",
    "docs = []\n",
    "metadata = []\n",
    "\n",
    "# === Handle Emotions ===\n",
    "for filename in os.listdir(emotion_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(emotion_dir, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            emotion = data.get(\"emotion\", \"unknown\")\n",
    "\n",
    "            for item in data.get(\"ayahs\", {}).get(\"primary\", []):\n",
    "                docs.append(f\"Ayah ({item['reference']}): {item['text_en']}\")\n",
    "                metadata.append({ \"type\": \"ayah\", \"category\": \"emotion\", \"tag\": emotion })\n",
    "\n",
    "            for item in data.get(\"hadiths\", {}).get(\"primary\", []):\n",
    "                docs.append(f\"Hadith: {item['text_en']}\")\n",
    "                metadata.append({ \"type\": \"hadith\", \"category\": \"emotion\", \"tag\": emotion })\n",
    "\n",
    "            for item in data.get(\"duas\", []):\n",
    "                docs.append(f\"Dua: {item['text_en']}\")\n",
    "                metadata.append({ \"type\": \"dua\", \"category\": \"emotion\", \"tag\": emotion })\n",
    "\n",
    "# === Handle Ibadah ===\n",
    "for filename in os.listdir(ibadah_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(ibadah_dir, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Skipping invalid or empty file: {filename}\")\n",
    "                continue\n",
    "\n",
    "            topic = data.get(\"topic\", \"unknown\")\n",
    "\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, list):\n",
    "                    for item in value:\n",
    "                        if isinstance(item, dict):\n",
    "                            text = item.get(\"text\") or item.get(\"text_en\")\n",
    "                            if text:\n",
    "                                docs.append(f\"{key.capitalize()}: {text}\")\n",
    "                                metadata.append({ \"type\": key, \"category\": \"ibadah\", \"topic\": topic })\n",
    "                        elif isinstance(item, str):\n",
    "                            docs.append(f\"{key.capitalize()}: {item}\")\n",
    "                            metadata.append({ \"type\": key, \"category\": \"ibadah\", \"topic\": topic })\n",
    "\n",
    "                elif isinstance(value, dict):\n",
    "                    for subkey, subvalue in value.items():\n",
    "                        if isinstance(subvalue, list):\n",
    "                            for subitem in subvalue:\n",
    "                                if isinstance(subitem, dict):\n",
    "                                    text = subitem.get(\"text\") or subitem.get(\"text_en\")\n",
    "                                    if text:\n",
    "                                        docs.append(f\"{key.capitalize()} - {subkey}: {text}\")\n",
    "                                        metadata.append({ \"type\": key, \"category\": \"ibadah\", \"topic\": topic })\n",
    "                                elif isinstance(subitem, str):\n",
    "                                    docs.append(f\"{key.capitalize()} - {subkey}: {subitem}\")\n",
    "                                    metadata.append({ \"type\": key, \"category\": \"ibadah\", \"topic\": topic })\n",
    "\n",
    "\n",
    "# === Encode and Index ===\n",
    "print(f\"Encoding {len(docs)} total docs...\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# === Save Index + Metadata ===\n",
    "with open(\"fiass_indexer.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"index\": index,\n",
    "        \"metadata\": metadata,\n",
    "        \"docs\": docs\n",
    "    }, f)\n",
    "\n",
    "print(\"Indexing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e45ece-3a03-44ef-84c1-f486ff62c877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize emotion detection classifier\n",
    "classifier = pipeline(\"text-classification\", model=\"nateraw/bert-base-uncased-emotion\")\n",
    "\n",
    "def detect_emotion(text):\n",
    "    \"\"\"\n",
    "    Detect the primary emotion in a text.\n",
    "    Returns a tuple: (label, score)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = classifier(text)[0]\n",
    "        label = result['label']  # e.g., 'sadness'\n",
    "        score = round(result['score'] * 100, 2)  # e.g., 97.2\n",
    "        return label, score\n",
    "    except Exception as e:\n",
    "        return \"unknown\", 0.0, f\"Emotion detection error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767a148e-4f96-4cb9-9fb3-febd51c0c4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2088\\2709926623.py:21: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key and base URL\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "\n",
    "# --- Load FAISS Index ---\n",
    "with open(\"fiass_indexer.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "index = data[\"index\"]\n",
    "metadata = data[\"metadata\"]\n",
    "docs = data[\"docs\"]\n",
    "\n",
    "# --- LLM Setup (DeepSeek via OpenRouter) ---\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek/deepseek-r1-0528:free\",\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=api_base\n",
    ")\n",
    "\n",
    "# --- Format Retrieved Context ---\n",
    "def format_context(docs, metadata, indices):\n",
    "    context_blocks = []\n",
    "    for idx in indices[0]:\n",
    "        meta = metadata[idx]\n",
    "        text = docs[idx]\n",
    "        block_type = meta.get(\"type\", \"\").lower()\n",
    "\n",
    "        if block_type == \"ayah\":\n",
    "            block = f\"üìñ Ayah ({meta.get('reference', 'unknown')}):\\n{text}\"\n",
    "        elif block_type == \"hadith\":\n",
    "            block = f\"üó£ Hadith ({meta.get('source', 'unknown')}):\\n{text}\"\n",
    "        elif block_type == \"dua\":\n",
    "            block = f\"ü§≤ Dua:\\n{text}\"\n",
    "        else:\n",
    "            block = f\"{text}\"\n",
    "\n",
    "        context_blocks.append(block)\n",
    "    return \"\\n\\n\".join(context_blocks)\n",
    "\n",
    "# --- Classify Query Intent ---\n",
    "def classify_query(user_input: str) -> str:\n",
    "    query = user_input.lower()\n",
    "    if any(w in query for w in [\"sad\", \"depressed\", \"alone\", \"hopeless\", \"anxious\", \"scared\", \"heart\", \"crying\"]):\n",
    "        return \"emotional\"\n",
    "    elif any(w in query for w in [\"how to pray\", \"how to fast\", \"explain wudu\", \"zakat\", \"hajj\", \"umrah\", \"ibadah\", \"tahajjud\"]):\n",
    "        return \"ibadah\"\n",
    "    elif any(w in query for w in [\"fiqh\", \"is it haram\", \"halal\", \"fatwa\", \"allowed in islam\", \"permissible\"]):\n",
    "        return \"fiqh\"\n",
    "    elif any(w in query for w in [\"tafsir\", \"ayah\", \"verse\", \"surah\", \"explain this ayah\", \"what does this mean in quran\"]):\n",
    "        return \"tafsir\"\n",
    "    elif any(w in query for w in [\"prophet\", \"story of\", \"life of\", \"nabi\", \"messenger\", \"who was\"]):\n",
    "        return \"story\"\n",
    "    else:\n",
    "        return \"general\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "793fc838-792c-4d90-be3d-32263f022d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Deen Buddy Function ---\n",
    "def deen_buddy(user_input: str, top_k=6):\n",
    "    try:\n",
    "        query_type = classify_query(user_input)\n",
    "        emotion, confidence = \"sadness\", 0.95  # Replace with actual model if available\n",
    "\n",
    "        # Embed and search FAISS index\n",
    "        query_embedding = embedding_model.encode([user_input])\n",
    "        distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "        formatted_context = format_context(docs, metadata, indices)\n",
    "\n",
    "        # === Prompts ===\n",
    "        if query_type == \"emotional\":\n",
    "            prompt = f\"\"\"\n",
    "You are Deen Buddy, a compassionate and wise Islamic friend.\n",
    "\n",
    "The user is feeling emotionally low (detected: {emotion.upper()}, confidence {confidence:.2f}).\n",
    "\n",
    "Here is some Islamic guidance for your reference:\n",
    "{formatted_context}\n",
    "\n",
    "Now, speak like a close friend ‚Äî warm, heartfelt, and understanding. Comfort them using beautiful reminders from Qur'an and Hadith. Avoid bullet points or headings. Just speak with love and wisdom.\n",
    "\"\"\"\n",
    "\n",
    "        elif query_type == \"ibadah\":\n",
    "            prompt = f\"\"\"\n",
    "The user wants to learn about an Ibadah topic (e.g., prayer, fasting, tahajjud).\n",
    "\n",
    "Use the following authentic Islamic material:\n",
    "{formatted_context}\n",
    "\n",
    "Respond like a friendly teacher helping someone new to the faith. Be warm, simple, and accurate. Include ayahs and hadiths as needed, but do not use bullet points. Just flow like you're having a natural conversation.\n",
    "\"\"\"\n",
    "\n",
    "        elif query_type == \"fiqh\":\n",
    "            return \"This seems like a fiqh-related question. It's best to consult a qualified Mufti or scholar, as fiqh can depend on specific madhabs and contexts. May Allah guide you!\"\n",
    "\n",
    "        elif query_type == \"tafsir\":\n",
    "            prompt = f\"\"\"\n",
    "The user asked for explanation of a Qur'anic ayah or surah.\n",
    "\n",
    "Use the context below if it helps:\n",
    "{formatted_context}\n",
    "\n",
    "Explain the ayah clearly and spiritually, based on authentic tafsir. Include the Arabic and a good English translation. No bullets. Explain gently, with wisdom.\n",
    "\"\"\"\n",
    "\n",
    "        elif query_type == \"story\":\n",
    "            prompt = f\"\"\"\n",
    "The user wants to hear a story from the life of the Prophets or companions.\n",
    "\n",
    "If the context below helps, you may use it:\n",
    "{formatted_context}\n",
    "\n",
    "Narrate the story like a loving friend ‚Äî make it feel real, warm, and spiritually uplifting. Don't list facts. Just flow with emotion and wisdom, using authentic details.\n",
    "\"\"\"\n",
    "\n",
    "        else:  # General\n",
    "            prompt = f\"\"\"\n",
    "User asked: \"{user_input}\"\n",
    "\n",
    "You are Deen Buddy ‚Äî a kind and knowledgeable Islamic companion. Use wisdom and gentle speech.\n",
    "\n",
    "If you can answer from Qur'an or Hadith, do so with references. If not, admit respectfully.\n",
    "\n",
    "Here is some reference context:\n",
    "{formatted_context}\n",
    "\n",
    "Respond naturally, without listing ‚Äî just a warm, conversational reply.\n",
    "\"\"\"\n",
    "\n",
    "        # Call LLM\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        return response.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26010a64-48e3-4df0-8178-766636857316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ Response from Deen Buddy:\n",
      "\n",
      "I‚Äôm truly sorry to hear you‚Äôre going through this hardship. Remember, even in moments of loss, Allah is near. The Prophet Yaqub (ÿπŸÑŸäŸá ÿßŸÑÿ≥ŸÑÿßŸÖ), in his deepest grief, turned to Allah and said, *‚ÄúI only complain of my suffering and my grief to Allah‚Äù* (Qur‚Äôan 12:86). Pour your heart out to Him ‚Äî He listens.  \n",
      "\n",
      "Allah promises, *‚ÄúBe patient, for He does not let the reward of the good-doers be lost‚Äù* (11:115). This trial may be a path to something greater. Trust His plan. When we lose something worldly, He replaces it with what‚Äôs better for our faith and life ‚Äî if we turn to Him.  \n",
      "\n",
      "The Prophet Ô∑∫ taught us to say in times of calamity: *‚ÄúInnƒÅ lillƒÅhi wa innƒÅ ilayhi rƒÅji‚Äô≈´n. Allahumma‚Äôjur-nƒ´ fƒ´ mu·π£ƒ´batƒ´‚Ä¶‚Äù* (O Allah, reward me and replace my loss with better). Say this often, and seek refuge in Him from grief, as taught in the du‚Äôa you shared.  \n",
      "\n",
      "Hold onto hope. Your rizq is in Allah‚Äôs Hands ‚Äî and He *never* closes one door without opening another, even if we can‚Äôt see it yet. You‚Äôre not alone. ü§≤\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Ask something: \")\n",
    "response = deen_buddy(query)\n",
    "print(\"\\nüí¨ Response from Deen Buddy:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d3f04-3a0f-4ed0-871e-436983c11397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (your_env_name)",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
